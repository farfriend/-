{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a26fb572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "扫地机器人在随机策略下的状态值：\n",
      "-1.111 -1.359 -1.615 -0.329 1.368 \n",
      "-1.417 -2.372 -4.369 -0.987 0.000 \n",
      "-1.831 -4.716 None -3.987 -0.300 \n",
      "-0.731 -2.162 -4.649 -2.160 -0.887 \n",
      "0.000 -0.716 -1.772 -1.280 -0.867 \n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "深度强化学习——原理、算法与PyTorch实战\n",
    "'''\n",
    "import numpy as np\n",
    "\n",
    "class sweeprobot():\n",
    "    def __init__(self):\n",
    "        # 状态空间\n",
    "        self.S = [[[0, 0], [0, 1], [0, 2], [0, 3], [0, 4], [0, 5]],\n",
    "                  [[1, 0], [1, 1], [1, 2], [1, 3], [1, 4], [1, 5]],\n",
    "                  [[2, 0], [2, 1], [2, 2], [2, 3], [2, 4], [2, 5]],\n",
    "                  [[3, 0], [3, 1], [3, 2], [3, 3], [3, 4], [4, 5]],\n",
    "                  [[4, 0], [4, 1], [4, 2], [4, 3], [4, 4], [4, 5]],\n",
    "                  [[5, 0], [5, 1], [5, 2], [5, 3], [5, 4], [5, 5]]]\n",
    "        # 动作空间\n",
    "        self.A = [[None, None], [-1, 0], [1, 0], [0, -1], [0, 1]]\n",
    "        # 状态值\n",
    "        self.V = [[None for i in range(6)] for j in range(6)]\n",
    "        self.V[1][1] = 0\n",
    "        self.V[5][4] = 0\n",
    "        # 策略\n",
    "        self.pi = None\n",
    "        self.gamma = 0.8\n",
    "\n",
    "    def reward(self, s, a):\n",
    "        # 奖励函数\n",
    "        [truth1, truth2] = np.add(s, a) == [5, 4]\n",
    "        [truth3, truth4] = np.add(s, a) == [1, 1]\n",
    "        [truth5, truth6] = np.add(s, a) == [3, 3]\n",
    "        # 若状态s转移到[5,4](收集垃圾)\n",
    "        if s != [5, 4] and (truth1 and truth2):\n",
    "            return 3\n",
    "        # 若状态s转移到[1,1](充电)\n",
    "        if s != [1, 1] and (truth3 and truth4):\n",
    "            return 1\n",
    "        # 若状态s转移到[3,3](撞到障碍物)\n",
    "        if truth5 and truth6:\n",
    "            return -10\n",
    "        return 0\n",
    "\n",
    "    def cal_coefficient(self):\n",
    "        # 该函数用来计算出线性方程组的系数矩阵和向量值\n",
    "        # 首先初始化一个25 * 25的系数矩阵和25个元素的向量\n",
    "        coef_Matrix = [[0 for i in range(25)] for j in range(25)]\n",
    "        b = [0 for i in range(25)]\n",
    "        for i in range(1, 6):\n",
    "            for j in range(1, 6):\n",
    "                # 判断是否是终止情况，如果是的话直接计算下一个\n",
    "                [truth1, truth2] = [i == 5, j == 4]\n",
    "                [truth3, truth4] = [i == 1, j == 1]\n",
    "                [truth5, truth6] = [i == 3, j == 3]\n",
    "                if truth1 and truth2:\n",
    "                    continue\n",
    "                if truth3 and truth4:\n",
    "                    continue\n",
    "                if truth5 and truth6:\n",
    "                    continue\n",
    "                # 计算当前状态下的动作数，以用于计算策略pi\n",
    "                count_action = 0\n",
    "                if i - 1 >= 1:\n",
    "                    count_action += 1\n",
    "                if i + 1 <= 5:\n",
    "                    count_action += 1\n",
    "                if j - 1 >= 1:\n",
    "                    count_action += 1\n",
    "                if j + 1 <= 5:\n",
    "                    count_action += 1\n",
    "                self.pi = 1 / count_action\n",
    "                # 具体计算每一个状态值的函数\n",
    "                b_value = 0\n",
    "                coef_CurrentState = 0\n",
    "                # 向上的情况\n",
    "                if i - 1 >= 1:\n",
    "                    b_value = b_value + self.pi * self.reward(self.S[i][j], self.A[1])\n",
    "                    if i - 1 == 3 and j == 3:\n",
    "                        coef_CurrentState = self.pi * self.gamma\n",
    "                    else:\n",
    "                        coef1 = self.pi * self.gamma\n",
    "                        coef_Matrix[(i - 1) * 5 + j - 1][((i - 1) - 1) * 5 + j - 1] = coef1\n",
    "                # 向下的情况\n",
    "                if i + 1 <= 5:\n",
    "                    b_value = b_value + self.pi * self.reward(self.S[i][j], self.A[2])\n",
    "                    if i + 1 == 3 and j == 3:\n",
    "                        coef_CurrentState = self.pi * self.gamma\n",
    "                    else:\n",
    "                        coef2 = self.pi * self.gamma\n",
    "                        coef_Matrix[(i - 1) * 5 + j - 1][((i + 1) - 1) * 5 + j - 1] = coef2\n",
    "                # 向左的情况\n",
    "                if j - 1 >= 1:\n",
    "                    b_value = b_value + self.pi * self.reward(self.S[i][j], self.A[3])\n",
    "                    if j - 1 == 3 and i == 3:\n",
    "                        coef_CurrentState = self.pi * self.gamma\n",
    "                    else:\n",
    "                        coef3 = self.pi * self.gamma\n",
    "                        coef_Matrix[(i - 1) * 5 + j - 1][(i - 1) * 5 + (j - 1) - 1] = coef3\n",
    "                # 向右的情况\n",
    "                if j + 1 <= 5:\n",
    "                    b_value = b_value + self.pi * self.reward(self.S[i][j], self.A[4])\n",
    "                    if j + 1 == 3 and i == 3:\n",
    "                        coef_CurrentState = self.pi * self.gamma\n",
    "                    else:\n",
    "                        coef4 = self.pi * self.gamma\n",
    "                        coef_Matrix[(i - 1) * 5 + j - 1][(i - 1) * 5 + (j + 1) - 1] = coef4\n",
    "                # 将左边的移项，所以系数为-1 (单位矩阵减系数矩阵)\n",
    "                coef_Matrix[(i - 1) * 5 + j - 1][(i - 1) * 5 + j - 1] = -1 + coef_CurrentState\n",
    "                # 同理，将常数项移项需要乘-1\n",
    "                b[(i - 1) * 5 + j - 1] = -1 * b_value\n",
    "        # 因为状态[1,1]和状态[5,4]可以确定其状态值为0,状态[3,3]不存在，所以其实只需求22*22的矩阵和22个元素的向量值\n",
    "        # 把矩阵和向量第[(1-1)*5+1-1]和[(5-1)*5+4-1]删除\n",
    "        del coef_Matrix[23]\n",
    "        del b[23]\n",
    "        del coef_Matrix[12]\n",
    "        del b[12]\n",
    "        del coef_Matrix[0]\n",
    "        del b[0]\n",
    "        # 把矩阵每一行的[(1-1)*5+1-1]和[(5-1)*5+4-1]和[(3-1)*5+3-1]删除\n",
    "        for item in coef_Matrix:\n",
    "            del item[23]\n",
    "            del item[12]\n",
    "            del item[0]\n",
    "        # 得到系数矩阵coef_Matrix = (γP-I)与 b = -R,其中γ为衰退因子，P为状态转移矩阵，I为单位矩阵，R为奖励函数\n",
    "        return coef_Matrix, b\n",
    "\n",
    "    def solve_equation(self, coef_Matrix, b):\n",
    "        # 计算状态值函数\n",
    "        # 解方程组A*x = b,其中A = (γP-I)，b = -R \n",
    "        A = np.array(coef_Matrix)\n",
    "        b = np.array(b)\n",
    "        x = np.linalg.solve(A, b)\n",
    "        x = list(x)\n",
    "        for i in range(1, 6):\n",
    "            for j in range(1, 6):\n",
    "                [truth1, truth2] = [i == 5, j == 4]\n",
    "                [truth3, truth4] = [i == 1, j == 1]\n",
    "                [truth5, truth6] = [i == 3, j == 3]\n",
    "                if truth1 and truth2:\n",
    "                    continue\n",
    "                if truth3 and truth4:\n",
    "                    continue\n",
    "                if truth5 and truth6:\n",
    "                    continue\n",
    "                self.V[i][j] = x.pop(0)\n",
    "\n",
    "    def print_value(self):\n",
    "        # 输出扫地机器人的状态值\n",
    "        print('扫地机器人在随机策略下的状态值：')\n",
    "        for i in range(1, 6):\n",
    "            for j in range(1, 6):\n",
    "                if self.V[j][6 - i] != None:\n",
    "                    print('%.3f'%self.V[j][6 - i], end=\" \")\n",
    "                else:\n",
    "                    print(self.V[j][6 - i], end=\" \")\n",
    "            print()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sr = sweeprobot()\n",
    "    A, b = sr.cal_coefficient()\n",
    "    sr.solve_equation(A, b)\n",
    "    sr.print_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41bfc783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "扫地机器人最优状态值：\n",
      "1.229 1.536 1.920 2.400 3.000 \n",
      "1.536 1.920 2.400 3.000 0.000 \n",
      "1.229 1.536 None 2.400 3.000 \n",
      "1.000 1.229 1.536 1.920 2.400 \n",
      "0.000 1.000 1.229 1.536 1.920 \n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "深度强化学习——原理、算法与PyTorch实战\n",
    "'''\n",
    "import numpy as np\n",
    "\n",
    "class SweepRobot():\n",
    "    def __init__(self):\n",
    "        # 状态空间\n",
    "        self.S = [[[0, 0], [0, 1], [0, 2], [0, 3], [0, 4], [0, 5]],\n",
    "                  [[1, 0], [1, 1], [1, 2], [1, 3], [1, 4], [1, 5]],\n",
    "                  [[2, 0], [2, 1], [2, 2], [2, 3], [2, 4], [2, 5]],\n",
    "                  [[3, 0], [3, 1], [3, 2], [3, 3], [3, 4], [4, 5]],\n",
    "                  [[4, 0], [4, 1], [4, 2], [4, 3], [4, 4], [4, 5]],\n",
    "                  [[5, 0], [5, 1], [5, 2], [5, 3], [5, 4], [5, 5]]]\n",
    "        # 动作空间\n",
    "        self.A = [[None, None], [-1, 0], [1, 0], [0, -1], [0, 1]]\n",
    "        # 状态值\n",
    "        self.V = [[None for i in range(6)] for j in range(6)]\n",
    "        self.V[1][1] = 0\n",
    "        self.V[5][4] = 0\n",
    "        # 无策略\n",
    "        self.gamma = 0.8\n",
    "        self.theta = 0.0001\n",
    "\n",
    "    def reward(self, s, a):\n",
    "        # 奖励函数\n",
    "        [truth1, truth2] = np.add(s, a) == [5, 4]\n",
    "        [truth3, truth4] = np.add(s, a) == [1, 1]\n",
    "        [truth5, truth6] = np.add(s, a) == [3, 3]\n",
    "        # 若状态s转移到[5,4](收集垃圾)\n",
    "        if s != [5, 4] and (truth1 and truth2):\n",
    "            return 3\n",
    "        # 若状态s转移到[1,1](充电)\n",
    "        if s != [1, 1] and (truth3 and truth4):\n",
    "            return 1\n",
    "        # 若状态s转移到[3,3](撞到障碍物)\n",
    "        if truth5 and truth6:\n",
    "            return -10\n",
    "        return 0\n",
    "\n",
    "    def cal_optimal_value(self):\n",
    "        # 建立V的副本\n",
    "        copy_V = self.V\n",
    "        # 首先初始化V值,便于计算，都初始化为0\n",
    "        for i in range(1, 6):\n",
    "            for j in range(1, 6):\n",
    "                # 判断是否是终止情况，如果是的话直接计算下一个\n",
    "                [truth1, truth2] = [i == 5, j == 4]\n",
    "                [truth3, truth4] = [i == 1, j == 1]\n",
    "                [truth5, truth6] = [i == 3, j == 3]\n",
    "                if truth1 and truth2:\n",
    "                    continue\n",
    "                if truth3 and truth4:\n",
    "                    continue\n",
    "                if truth5 and truth6:\n",
    "                    continue\n",
    "                self.V[i][j] = 0\n",
    "                copy_V[i][j] = 0\n",
    "        while True:\n",
    "            Delta = 0\n",
    "            for i in range(1, 6):\n",
    "                for j in range(1, 6):\n",
    "                    # 判断是否是终止情况，如果是的话直接计算下一个\n",
    "                    [truth1, truth2] = [i == 5, j == 4]\n",
    "                    [truth3, truth4] = [i == 1, j == 1]\n",
    "                    [truth5, truth6] = [i == 3, j == 3]\n",
    "                    if truth1 and truth2:\n",
    "                        continue\n",
    "                    if truth3 and truth4:\n",
    "                        continue\n",
    "                    if truth5 and truth6:\n",
    "                        continue\n",
    "                    v = self.V[i][j]\n",
    "                    # 因为每个状态的动作空间不一样，所以需要分情况讨论\n",
    "                    max_value = 0\n",
    "                    # 向上的情况\n",
    "                    if i - 1 >= 1:\n",
    "                        if i - 1 == 3 and j == 3:\n",
    "                            max_value = max(max_value, self.reward(self.S[i][j], self.A[1]) + self.gamma * self.V[i][j])\n",
    "                        else:\n",
    "                            max_value = max(max_value, self.reward(self.S[i][j], self.A[1]) + self.gamma * self.V[i - 1][j])\n",
    "                    # 向下的情况\n",
    "                    if i + 1 <= 5:\n",
    "                        if i + 1 == 3 and j == 3:\n",
    "                            max_value = max(max_value, self.reward(self.S[i][j], self.A[1]) + self.gamma * self.V[i][j])\n",
    "                        else:\n",
    "                            max_value = max(max_value, self.reward(self.S[i][j], self.A[2]) + self.gamma * self.V[i + 1][j])\n",
    "                    # 向左的情况\n",
    "                    if j - 1 >= 1:\n",
    "                        if j - 1 == 3 and i == 3:\n",
    "                            max_value = max(max_value, self.reward(self.S[i][j], self.A[1]) + self.gamma * self.V[i][j])\n",
    "                        else:\n",
    "                            max_value = max(max_value, self.reward(self.S[i][j], self.A[3]) + self.gamma * self.V[i][j - 1])\n",
    "                    # 向右的情况\n",
    "                    if j + 1 <= 5:\n",
    "                        if j + 1 == 3 and i == 3:\n",
    "                            max_value = max(max_value, self.reward(self.S[i][j], self.A[1]) + self.gamma * self.V[i][j])\n",
    "                        else:\n",
    "                            max_value = max(max_value, self.reward(self.S[i][j], self.A[4]) + self.gamma * self.V[i][j + 1])\n",
    "                    copy_V[i][j] = max_value\n",
    "                    Delta = max(Delta, abs(v - copy_V[i][j]))\n",
    "            self.V = copy_V\n",
    "            if Delta < self.theta:\n",
    "                break\n",
    "\n",
    "    def print_value(self):\n",
    "        # 输出扫地机器人的状态值\n",
    "        print('扫地机器人最优状态值：')\n",
    "        for i in range(1, 6):\n",
    "            for j in range(1, 6):\n",
    "                if self.V[j][6 - i] != None:\n",
    "                    print('%.3f'%self.V[j][6 - i], end=\" \")\n",
    "                else:\n",
    "                    print(self.V[j][6 - i], end=\" \")\n",
    "            print()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sr = SweepRobot()\n",
    "    sr.cal_optimal_value()\n",
    "    sr.print_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd11c7f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4372\\1830632278.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# 添加当前目录到路径，确保可以导入自定义模块\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# 尝试导入自定义的 GridWorldEnv，如果失败则使用我们之前定义的类\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "# 代05-例4.1-基于状态值函数的确定环境扫地机器人任务策略评估\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# 添加当前目录到路径，确保可以导入自定义模块\n",
    "sys.path.append(os.path.dirname(__file__))\n",
    "\n",
    "# 尝试导入自定义的 GridWorldEnv，如果失败则使用我们之前定义的类\n",
    "try:\n",
    "    from book_gridword import GridWorldEnv\n",
    "    print(\"成功导入 book_gridword 模块\")\n",
    "except ImportError:\n",
    "    print(\"无法导入 book_gridword，使用内置的 GridWorldEnv 类\")\n",
    "    # 这里放置我们之前定义的完整 GridWorldEnv 类\n",
    "    import gymnasium as gym\n",
    "    from gymnasium import spaces\n",
    "    from gymnasium.utils import seeding\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.patches as patches\n",
    "    \n",
    "    class Grid(object):\n",
    "        def __init__(self, x: int = None, y: int = None, grid_type: int = 0, enter_reward: float = 0.0):\n",
    "            self.x = x\n",
    "            self.y = y\n",
    "            self.grid_type = grid_type\n",
    "            self.enter_reward = enter_reward\n",
    "            self.name = \"X{0}-Y{1}\".format(self.x, self.y)\n",
    "    \n",
    "    class GridMatrix(object):\n",
    "        def __init__(self, n_width: int, n_height: int, default_type: int = 0, default_reward: float = 0.0):\n",
    "            self.n_height = n_height\n",
    "            self.n_width = n_width\n",
    "            self.default_reward = default_reward\n",
    "            self.default_type = default_type\n",
    "            self.grids = None\n",
    "            self.len = n_width * n_height\n",
    "            self.reset()\n",
    "        \n",
    "        def reset(self):\n",
    "            self.grids = []\n",
    "            for x in range(self.n_height):\n",
    "                for y in range(self.n_width):\n",
    "                    self.grids.append(Grid(x, y, self.default_type, self.default_reward))\n",
    "        \n",
    "        def get_grid(self, x, y=None):\n",
    "            xx, yy = None, None\n",
    "            if isinstance(x, int):\n",
    "                xx, yy = x, y\n",
    "            elif isinstance(x, tuple):\n",
    "                xx, yy = x[0], x[1]\n",
    "            assert (0 <= xx < self.n_width and 0 <= yy < self.n_height)\n",
    "            index = yy * self.n_width + xx\n",
    "            return self.grids[index]\n",
    "        \n",
    "        def set_reward(self, x, y, reward):\n",
    "            grid = self.get_grid(x, y)\n",
    "            if grid is not None:\n",
    "                grid.enter_reward = reward\n",
    "        \n",
    "        def set_type(self, x, y, grid_type):\n",
    "            grid = self.get_grid(x, y)\n",
    "            if grid is not None:\n",
    "                grid.grid_type = grid_type\n",
    "        \n",
    "        def get_reward(self, x, y):\n",
    "            grid = self.get_grid(x, y)\n",
    "            if grid is None:\n",
    "                return None\n",
    "            return grid.enter_reward\n",
    "        \n",
    "        def get_type(self, x, y):\n",
    "            grid = self.get_grid(x, y)\n",
    "            if grid is None:\n",
    "                return None\n",
    "            return grid.grid_type\n",
    "    \n",
    "    class GridWorldEnv(gym.Env):\n",
    "        metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 30}\n",
    "        \n",
    "        def __init__(self, n_width: int = 5, n_height: int = 5, u_size=40, default_reward: float = 0, default_type=0):\n",
    "            self.n_width = n_width\n",
    "            self.n_height = n_height\n",
    "            self.default_reward = default_reward\n",
    "            self.default_type = default_type\n",
    "            self.u_size = u_size\n",
    "            \n",
    "            self.grids = GridMatrix(n_width=self.n_width, n_height=self.n_height,\n",
    "                                   default_reward=self.default_reward, default_type=self.default_type)\n",
    "            self.reward = 0\n",
    "            self.action = None\n",
    "            \n",
    "            self.action_space = spaces.Discrete(4)\n",
    "            self.observation_space = spaces.Discrete(self.n_height * self.n_width)\n",
    "            \n",
    "            self.state = None\n",
    "            self.ends = [(0, 0), (4, 3)]\n",
    "            self.start = (0, 4)\n",
    "            self.types = [(2, 2, 1)]\n",
    "            self.rewards = [(0, 0, 1), (4, 3, 5), (2, 2, -10)]\n",
    "            self.refresh_setting()\n",
    "            self.viewer = None\n",
    "            self.seed()\n",
    "            self.reset()\n",
    "        \n",
    "        def seed(self, seed=None):\n",
    "            self.np_random, seed = seeding.np_random(seed)\n",
    "            return [seed]\n",
    "        \n",
    "        def step(self, action):\n",
    "            assert self.action_space.contains(action), \"%r (%s) invalid\" % (action, type(action))\n",
    "            self.action = action\n",
    "            old_x, old_y = self._state_to_xy(self.state)\n",
    "            new_x, new_y = old_x, old_y\n",
    "            \n",
    "            if action == 0: new_x -= 1  # left\n",
    "            elif action == 1: new_x += 1  # right\n",
    "            elif action == 2: new_y += 1  # up\n",
    "            elif action == 3: new_y -= 1  # down\n",
    "            \n",
    "            if new_x < 0: new_x = 0\n",
    "            if new_x >= self.n_width: new_x = self.n_width - 1\n",
    "            if new_y < 0: new_y = 0\n",
    "            if new_y >= self.n_height: new_y = self.n_height - 1\n",
    "            \n",
    "            if self.grids.get_type(new_x, new_y) == 1:\n",
    "                new_x, new_y = old_x, old_y\n",
    "            \n",
    "            self.reward = self.grids.get_reward(new_x, new_y)\n",
    "            done = self._is_end_state(new_x, new_y)\n",
    "            self.state = self._xy_to_state(new_x, new_y)\n",
    "            info = {\"x\": new_x, \"y\": new_y, \"grids\": self.grids}\n",
    "            terminated = done\n",
    "            truncated = False\n",
    "            return self.state, self.reward, terminated, truncated, info\n",
    "        \n",
    "        def _state_to_xy(self, s):\n",
    "            x = s % self.n_width\n",
    "            y = int((s - x) / self.n_width)\n",
    "            return x, y\n",
    "        \n",
    "        def _xy_to_state(self, x, y=None):\n",
    "            if isinstance(x, int):\n",
    "                return x + self.n_width * y\n",
    "            elif isinstance(x, tuple):\n",
    "                return x[0] + self.n_width * x[1]\n",
    "            return -1\n",
    "        \n",
    "        def refresh_setting(self):\n",
    "            for x, y, r in self.rewards:\n",
    "                self.grids.set_reward(x, y, r)\n",
    "            for x, y, t in self.types:\n",
    "                self.grids.set_type(x, y, t)\n",
    "        \n",
    "        def reset(self, seed=None, options=None):\n",
    "            if seed is not None:\n",
    "                self.seed(seed)\n",
    "            self.state = self._xy_to_state(self.start)\n",
    "            info = {}\n",
    "            return self.state, info\n",
    "        \n",
    "        def _is_end_state(self, x, y=None):\n",
    "            if y is not None:\n",
    "                xx, yy = x, y\n",
    "            elif isinstance(x, int):\n",
    "                xx, yy = self._state_to_xy(x)\n",
    "            else:\n",
    "                assert (isinstance(x, tuple)), \"坐标数据不完整\"\n",
    "                xx, yy = x[0], x[1]\n",
    "            for end in self.ends:\n",
    "                if xx == end[0] and yy == end[1]:\n",
    "                    return True\n",
    "            return False\n",
    "        \n",
    "        def render(self):\n",
    "            fig, ax = plt.subplots(figsize=(10, 10))\n",
    "            \n",
    "            for x in range(self.n_width):\n",
    "                for y in range(self.n_height):\n",
    "                    grid_type = self.grids.get_type(x, y)\n",
    "                    reward = self.grids.get_reward(x, y)\n",
    "                    \n",
    "                    color = 'white'\n",
    "                    if grid_type == 1: color = 'gray'\n",
    "                    elif (x, y) in self.ends: color = 'gold'\n",
    "                    elif (x, y) == self.start: color = 'lightblue'\n",
    "                    elif reward > 0: color = 'lightgreen'\n",
    "                    elif reward < 0: color = 'lightcoral'\n",
    "                    \n",
    "                    rect = patches.Rectangle((x, y), 1, 1, linewidth=2, edgecolor='black', facecolor=color, alpha=0.8)\n",
    "                    ax.add_patch(rect)\n",
    "                    \n",
    "                    if reward != 0:\n",
    "                        ax.text(x + 0.5, y + 0.5, f'{reward}', ha='center', va='center', \n",
    "                               fontsize=14, fontweight='bold',\n",
    "                               bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='white', alpha=0.8))\n",
    "            \n",
    "            agent_x, agent_y = self._state_to_xy(self.state)\n",
    "            agent_circle = plt.Circle((agent_x + 0.5, agent_y + 0.5), 0.3, color='red', alpha=0.8, zorder=10)\n",
    "            ax.add_patch(agent_circle)\n",
    "            \n",
    "            ax.set_xlim(-0.5, self.n_width + 0.5)\n",
    "            ax.set_ylim(-0.5, self.n_height + 0.5)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xticks(range(self.n_width + 1))\n",
    "            ax.set_yticks(range(self.n_height + 1))\n",
    "            ax.grid(True, linestyle='-', linewidth=1, alpha=0.3)\n",
    "            ax.set_title('扫地机器人环境 - 策略评估', fontsize=16, fontweight='bold', pad=20)\n",
    "            ax.set_xlabel('X 坐标', fontsize=12)\n",
    "            ax.set_ylabel('Y 坐标', fontsize=12)\n",
    "            \n",
    "            legend_elements = [\n",
    "                plt.Rectangle((0,0),1,1, facecolor='lightblue', edgecolor='black', label='起点'),\n",
    "                plt.Rectangle((0,0),1,1, facecolor='gold', edgecolor='black', label='终点'),\n",
    "                plt.Rectangle((0,0),1,1, facecolor='gray', edgecolor='black', label='障碍物'),\n",
    "                plt.Rectangle((0,0),1,1, facecolor='lightgreen', edgecolor='black', label='正奖励'),\n",
    "                plt.Rectangle((0,0),1,1, facecolor='lightcoral', edgecolor='black', label='负奖励'),\n",
    "                plt.Circle((0,0),0.3, facecolor='red', edgecolor='black', label='智能体')\n",
    "            ]\n",
    "            ax.legend(handles=legend_elements, loc='upper right', bbox_to_anchor=(1.15, 1), fontsize=10)\n",
    "            ax.invert_yaxis()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            return fig, ax\n",
    "        \n",
    "        def close(self):\n",
    "            if hasattr(self, 'viewer') and self.viewer is not None:\n",
    "                plt.close('all')\n",
    "\n",
    "# 创建环境实例\n",
    "env = GridWorldEnv()\n",
    "\n",
    "\"\"\"定义格子世界参数\"\"\"\n",
    "world_h = 5\n",
    "world_w = 5\n",
    "length = world_h * world_w\n",
    "gamma = 0.8\n",
    "state = [i for i in range(length)]  # 状态（编号）\n",
    "action = ['n', 's', 'w', 'e']  # 动作名称\n",
    "ds_action = {'n': -world_w, 'e': 1, 's': world_w, 'w': -1}\n",
    "policy = np.zeros([length, len(action)])\n",
    "suqe = [20, 21, 22, 23, 24, 15, 16, 17, 18, 19, 10, 11, 12, 13, 14, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4]\n",
    "\n",
    "# 定义奖励函数 - 注意：这里的奖励设置与GridWorldEnv中的不同\n",
    "def reward(s):\n",
    "    if s == 20:  # 到充电站\n",
    "        return 1\n",
    "    elif s == 12:  # 到陷阱中\n",
    "        return -10\n",
    "    elif s == 9:  # 到垃圾处\n",
    "        return 3\n",
    "    else:\n",
    "        return 0  # 其他\n",
    "\n",
    "# 将动作名称转换为数字索引\n",
    "def getAction(a):\n",
    "    if a == 'n':\n",
    "        return 0\n",
    "    elif a == 'e':\n",
    "        return 3\n",
    "    elif a == 's':\n",
    "        return 1\n",
    "    elif a == 'w':\n",
    "        return 2\n",
    "\n",
    "# 在s状态下执行动作a，返回下一状态（编号）\n",
    "def next_states(s, a):\n",
    "    # 越过边界时pass\n",
    "    if (s < world_w and a == 'n') \\\n",
    "            or (s % world_w == 0 and a == 'w') \\\n",
    "            or (s > length - world_w - 1 and a == 's') \\\n",
    "            or ((s + 1) % world_w == 0 and a == 'e'):\n",
    "        next_state = s  # 表现为next_state不变\n",
    "    else:\n",
    "        next_state = s + ds_action[a]  # 进入下一个状态\n",
    "    return next_state\n",
    "\n",
    "# 在s状态下执行动作，返回所有可能的下一状态（编号）list\n",
    "def getsuccessor(s):\n",
    "    successor = []\n",
    "    for a in action:  # 遍历四个动作\n",
    "        if s == next_states(s, a):\n",
    "            continue\n",
    "        else:\n",
    "            next = next_states(s, a)  # 得到下一个状态（编号）\n",
    "        successor.append(next)  # 以list保存当前状态s下执行四个动作的下一状态\n",
    "    return successor\n",
    "\n",
    "# 初始化策略 - 均匀随机策略\n",
    "def initPolicy():\n",
    "    for s in range(length):\n",
    "        for a in action:\n",
    "            if next_states(s, a) == s:\n",
    "                continue\n",
    "            newAction = getAction(a)\n",
    "            policy[s][newAction] = 1 / len(getsuccessor(s))\n",
    "    print(\"策略初始化完成\")\n",
    "    print(\"策略矩阵形状:\", policy.shape)\n",
    "\n",
    "# 策略评估函数\n",
    "def policy_eval(theta=0.0001):\n",
    "    V = np.zeros(length)  # 初始化状态值函数列表\n",
    "    iter = 0\n",
    "\n",
    "    while True:\n",
    "        k = -1\n",
    "        delta = 0  # 定义最大差值，判断是否有进行更新\n",
    "        \n",
    "        print(f\"\\n=== 第 {iter+1} 次迭代 ===\")\n",
    "        \n",
    "        for s in suqe:  # 遍历所有状态 [20,21,...,0,1,2,3,4]\n",
    "            k += 1\n",
    "            if s in [9, 20, 12]:  # 若当前状态为终止状态，则直接pass不做操作\n",
    "                continue\n",
    "                \n",
    "            v = 0  # 针对每个状态值函数进行计算\n",
    "            print(f\"状态 {s}: \", end=\"\")\n",
    "            \n",
    "            # 遍历所有可能的动作\n",
    "            for a in action:\n",
    "                newAction = getAction(a)\n",
    "                next_state = next_states(s, a)\n",
    "                rewards = reward(next_state)\n",
    "                \n",
    "                # 计算值函数\n",
    "                if next_state == 12:  # 特殊处理陷阱状态\n",
    "                    v += policy[s][newAction] * (rewards + gamma * V[s])\n",
    "                    print(\" %.2f*(%d+%.1f*%.3f)+\" % (policy[s][newAction], rewards, gamma, V[next_state]), end=\"\")\n",
    "                else:\n",
    "                    v += policy[s][newAction] * (rewards + gamma * V[next_state])\n",
    "                    print(\" %.2f*(%d+%.1f*%.3f)+\" % (policy[s][newAction], rewards, gamma, V[next_state]), end=\"\")\n",
    "            \n",
    "            print(\" => v = %.3f\" % (v))\n",
    "            delta = max(delta, np.abs(v - V[s]))  # 更新差值\n",
    "            V[s] = v  # 存储(更新)每个状态下的状态值函数\n",
    "        \n",
    "        # 将值函数重塑为网格形式并显示\n",
    "        value = np.array(V).reshape(world_h, world_w)\n",
    "        iter += 1\n",
    "        \n",
    "        print('\\n迭代次数 k =', iter)\n",
    "        print(\"当前的状态值函数为：\")\n",
    "        print(np.round(value, decimals=3))\n",
    "        print(f\"最大变化量 delta = {delta:.6f}\")\n",
    "        \n",
    "        if delta < theta:  # 收敛判断\n",
    "            print(f\"\\n策略评估收敛！共迭代 {iter} 次\")\n",
    "            break\n",
    "            \n",
    "        if iter >= 100:  # 防止无限循环\n",
    "            print(\"达到最大迭代次数，强制停止\")\n",
    "            break\n",
    "            \n",
    "    return V  # 返回最终的状态值函数\n",
    "\n",
    "# 可视化值函数\n",
    "def plot_value_function(V):\n",
    "    value_grid = np.array(V).reshape(world_h, world_w)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    im = ax.imshow(value_grid, cmap='viridis', interpolation='nearest')\n",
    "    \n",
    "    # 添加数值标签\n",
    "    for i in range(world_h):\n",
    "        for j in range(world_w):\n",
    "            text = ax.text(j, i, f'{value_grid[i, j]:.2f}',\n",
    "                          ha=\"center\", va=\"center\", color=\"w\", fontweight='bold')\n",
    "    \n",
    "    # 设置坐标轴\n",
    "    ax.set_xticks(np.arange(world_w))\n",
    "    ax.set_yticks(np.arange(world_h))\n",
    "    ax.set_xticklabels(np.arange(world_w))\n",
    "    ax.set_yticklabels(np.arange(world_h))\n",
    "    ax.set_xlabel('X 坐标')\n",
    "    ax.set_ylabel('Y 坐标')\n",
    "    ax.set_title('状态值函数 V(s)')\n",
    "    \n",
    "    # 添加颜色条\n",
    "    cbar = ax.figure.colorbar(im, ax=ax)\n",
    "    cbar.ax.set_ylabel('值函数大小', rotation=-90, va=\"bottom\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 主程序\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 60)\n",
    "    print(\"基于状态值函数的确定环境扫地机器人任务策略评估\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 初始化策略\n",
    "    initPolicy()\n",
    "    \n",
    "    # 执行策略评估\n",
    "    print(\"\\n开始策略评估...\")\n",
    "    final_values = policy_eval(theta=0.001)\n",
    "    \n",
    "    # 可视化最终的值函数\n",
    "    print(\"\\n最终状态值函数:\")\n",
    "    final_value_grid = np.array(final_values).reshape(world_h, world_w)\n",
    "    print(np.round(final_value_grid, decimals=3))\n",
    "    \n",
    "    # 绘制值函数热力图\n",
    "    print(\"\\n生成值函数可视化...\")\n",
    "    plot_value_function(final_values)\n",
    "    \n",
    "    # 使用环境渲染显示最终状态\n",
    "    print(\"\\n显示环境状态...\")\n",
    "    env.reset()\n",
    "    env.render()\n",
    "    \n",
    "    print(\"\\n策略评估完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5081953b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DQN",
   "language": "python",
   "name": "dqn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
